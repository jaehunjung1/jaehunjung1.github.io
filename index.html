<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jaehun Jung</title>

    <meta name="author" content="Jaehun Jung">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jaehun Jung
                </p>
                <p>I'm a Ph.D student in computer science at the <a href="https://www.cs.washington.edu/" target=”_blank”>University of Washington</a>, advised by <a href="https://homes.cs.washington.edu/~yejin/" target=”_blank”>Yejin Choi</a>. I am also a part-time student researcher in <a href="https://www.nvidia.com/en-us/research/">Nvidia Research</a>.</p>

                <p>My research focuses on <em>how to train and evaluate a model with a model, with minimal human supervision</em>. I am specifically excited in</p>

                <li><strong>Data Synthesis and Data Selection with Language Models</strong>: How do we define good data? Can we use this insight to generate synthetic data that are diverse, correct, and are helpful to model generalization?</li>
                <li><strong>Science of Automated Evaluation</strong>: Can we use a model to reliably evaluate other models? How can we guarantee the automated rubrics will align with ours?</li>

<!--                <ul>-->
<!--                  <li>Data Synthesis / Selection with Language Models: </li>-->
<!--                  <li>Improving Reliability and Controllability of Language Models</li>-->
<!--                  <li>LLM-based Reasoning</li>-->
<!--                  <li>Behavioral Analysis of LLMs</li>-->
<!--                </ul>-->
                <p>
                  Previously I was an undergrad at Seoul National University, advised by Professor <a href="https://datalab.snu.ac.kr/~ukang/" target=”_blank”>U Kang</a> and <a href="http://hcil.snu.ac.kr/people/jinwook-seo" target=”_blank”>Jinwook Seo</a>. I was also a part-time researcher in Kakao Enterprise, where I worked on knowledge-grounded dialogue agents.
                </p>
                <p style="text-align:center">
                  <a href="mailto:hoony123@cs.washington.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/jaehunjung_cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=_bXzUGEAAAAJ&hl=en" target=”_blank”>Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jaehunjung_com" target=”_blank”>Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jaehunjung1/" target=”_blank”>Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" />
                <img src='images/prismatic_synthesis.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="image_device" style="align-content: center;margin-bottom: 10pt">
                <img src='images/prismatic_synthesis.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
              </div>
              <a href="https://nvlabs.github.io/prismatic-synthesis" target=”_blank”>
                <span class="papertitle">Prismatic Synthesis & G-Vendi Score: How Data Diversification makes R1-32B a Better Teacher than R1-671B</span>
              </a>
              <br>
              <strong>Jaehun Jung</strong>,
              <a href="https://seungjuhan.me/">Seungju Han*</a>,
              <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/">Ximing Lu*</a>,
              <a href="https://skylerhallinan.com/" target=”_blank”>Skyler Hallinan*</a>,
              <a href="#">Shrimai Prabhumoye</a>,
              <a href="#">Mostafa Patwary</a>,
              <a href="#">Mohammad Shoeybi</a>,
              <a href="#">Bryan Catanzaro</a>,
              <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
              <br>
              <em>preprint</em>, 2025
              <br>
              <a href="https://nvlabs.github.io/prismatic-synthesis" target=”_blank”>blog</a>
              <p></p>
              <p>
                We show that data diversity (measured by our proposed metric G-Vendi) strongly predicts how the model
                generalizes after training. We leverage this finding to strategically diversify synthetic reasoning data.
                Our resulting datasets, despite generated by 32B LLM, leads to better performance in OOD than R1-671B
                generated & human-verified datasets.
              </p>
            </td>
          </tr>

          <tr>
            <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" />
                <img src='images/retro-search.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="image_device" style="align-content: center;margin-bottom: 10pt">
                <img src='images/retro-search.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
              </div>
              <a href="https://arxiv.org/abs/2504.04383" target=”_blank”>
                <span class="papertitle">Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning.</span>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/">Ximing Lu*</a>,
              <a href="https://seungjuhan.me/">Seungju Han*</a>,
              <a href="#">David Acuna Marrero*</a>,
              <a href="https://hyunw.kim/">Hyunwoo Kim*</a>
              <strong>Jaehun Jung*</strong>,
              <a href="#">Shrimai Prabhumoye</a>,
              <a href="https://muennighoff.github.io/">Niklas Muennighoff</a>,
              <a href="#">Mostafa Patwary</a>,
              <a href="#">Mohammad Shoeybi</a>,
              <a href="#">Bryan Catanzaro</a>,
              <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
              <br>
              <em>preprint</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2504.04383" target=”_blank”>paper</a>
              /
              <a href="data/retro-search.txt" target=”_blank”>bibtex</a>
              <p></p>
              <p>
                Search-guided distillation mitigates under-thinking & over-thinking of reasoning models, while simultaneously improving the accuracy.
              </p>
            </td>
          </tr>

          <tr>
            <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cascaded_selective_evaluation_image' />
                <img src='images/cascaded-selective-evaluation.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <div class="image_device" style="align-content: center;margin-bottom: 10pt">
                <img src='images/cascaded-selective-evaluation.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
              </div>
              <a href="https://arxiv.org/abs/2407.18370" target=”_blank”>
                <span class="papertitle">Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</span>
              </a>
              <br>
              <strong>Jaehun Jung</strong>,
              <a href="https://fabrahman.github.io/">Faeze Brahman</a>,
              <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
              <br>
              <em>ICLR</em>, 2025 <strong style="color:red">(Oral, Top 1.8%)</strong>
              <br>
              <a href="https://arxiv.org/abs/2407.18370" target=”_blank”>paper</a>
              /
              <a href="data/cascaded_selective_evaluation.txt" target=”_blank”>bibtex</a>
              <p></p>
              <p>
                We enhance LLM judges with a statistically rigorous guarantee of human agreement. We further extend this guarantee to propose Cascaded Selective Evaluation, where we start from a small cost-effective model as a judge, and escalate to a stronger model only when necessary&mdash;all while guaranteeing high agreement with humans.
              </p>
            </td>
          </tr>

      <tr>
        <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='infosumm_image' />
            <img src='images/infosumm.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <div class="image_device" style="align-content: center;margin-bottom: 10pt" >
            <img src='images/infosumm.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
          </div>
          <a href="https://arxiv.org/pdf/2403.13780" target=”_blank”>
            <span class="papertitle">Information-Theoretic Distillation for Reference-less Summarization</span>
          </a>
          <br>
          <strong>Jaehun Jung</strong>,
          <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/">Ximing Lu</a>,
          <a href="https://liweijiang.me/">Liwei Jiang</a>,
          <a href="https://fabrahman.github.io/">Faeze Brahman</a>,
          <a href="https://homes.cs.washington.edu/~pawest/">Peter West</a>,
          <a href="https://koh.pw/">Pang Wei Koh</a>,
          <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
          <br>
          <em>COLM</em>, 2024
          <br>
          <a href="https://arxiv.org/pdf/2403.13780" target=”_blank”>paper</a>
          /
          <a href="data/infosumm.txt" target=”_blank”>bibtex</a>
          <p></p>
          <p>
            Can small models excel at summarization without imitating LLM or human-written references? We present InfoSumm, a framework to distill a powerful summarizer that outperforms order-of-magnitude larger LLM summarizers, solely based on the information-theoretic objective for summarization.
          </p>
      </td>
    </tr>


    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='impossible_distillation_image' />
          <img src='images/impossible_distillation.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/impossible_distillation.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://arxiv.org/abs/2305.16635" target=”_blank”>
          <span class="papertitle">Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Models</span>
        </a>
        <br>
        <strong>Jaehun Jung</strong>,
        <a href="https://homes.cs.washington.edu/~pawest/" target=”_blank”>Peter West</a>,
        <a href="https://liweijiang.me/" target=”_blank”>Liwei Jiang</a>,
        <a href="https://fabrahman.github.io/" target=”_blank”>Faeze Brahman</a>,
        <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/" target=”_blank”>Ximing Lu</a>,
        <a href="https://jillianfisher.owlstown.net/" target=”_blank”>Jillian Fisher</a>,
        <a href="https://tsor13.github.io/" target=”_blank”>Taylor Sorensen</a>,
        <a href="https://homes.cs.washington.edu/~yejin/" target=”_blank”>Yejin Choi</a>
        <br>
        <em>NAACL</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2305.16635" target=”_blank”>paper</a>
        /
        <a href="https://huggingface.co/datasets/Jaehun/DIMPLE" target=”_blank”>data</a>
        /
        <a href="data/impossible_distillation.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          It is possible to generate a high-quality dataset for sentential paraphrasing and summarization directly from an off-the-shelf LM, even when it is impossible for the LM itself to reliably perform these tasks.
        </p>
      </td>
    </tr>



    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='jamdec_image' />
          <img src='images/jamdec.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/jamdec.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://arxiv.org/abs/2402.08761" target=”_blank”>
          <span class="papertitle">Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models</span>
        </a>
        <br>
        <a href="https://jillianfisher.owlstown.net/ target=”_blank”">Jillian Fisher</a>,
        <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/" target=”_blank”>Ximing Lu</a>,
        <strong>Jaehun Jung</strong>,
        <a href="https://liweijiang.me/" target=”_blank”>Liwei Jiang</a>,
        <a href="https://sites.google.com/uw.edu/zaid-harchaoui/main" target=”_blank”>Zaid Harchaoui</a>,
        <a href="https://homes.cs.washington.edu/~yejin/" target=”_blank”>Yejin Choi</a>
        <br>
        <em>NAACL</em>, 2024 <strong style="color:red">(Oral Presentation)</strong>
        <br>
        <a href="https://arxiv.org/abs/2402.08761" target=”_blank”>paper</a>
        /
        <a href="https://github.com/jfisher52/JAMDecoding" target=”_blank”>github</a>
        /
        <a href="data/jamdec.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          We introduce JamDec, an inference-time algorithm for authorship obfuscation that is domain-agnostic, controllable, yet does not require human supervision.
        </p>
      </td>
    </tr>


    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ipa_image' />
          <img src='images/ipa.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/ipa.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://aclanthology.org/2023.emnlp-main.424/" target=”_blank”>
          <span class="papertitle">Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/" target=”_blank”>Ximing Lu</a>,
        <a href="https://fabrahman.github.io/" target=”_blank”>Faeze Brahman</a>,
        <a href="https://homes.cs.washington.edu/~pawest/" target=”_blank”>Peter West</a>,
        <strong>Jaehun Jung</strong>,
        ...,
        <a href="https://www.seanre.com/" target=”_blank”>Xiang Ren</a>,
        <a href="https://wellecks.com/" target=”_blank”>Sean Welleck</a>,
        <a href="https://homes.cs.washington.edu/~yejin/" target=”_blank”>Yejin Choi</a>
        <br>
        <em>EMNLP</em>, 2023
        <br>
        <a href="https://aclanthology.org/2023.emnlp-main.424.pdf" target=”_blank”>paper</a>
        /
        <a href="https://github.com/GXimingLu/IPA" target=”_blank”>github</a>
        /
        <a href="data/ipa.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          Can we adapt LLMs without fine-tuning? We propose using a lightweight adapter (e.g. GPT-2) during decoding time, efficiently tailoring even the strongest proprietary LLMs toward user-defined reward.
        </p>
      </td>
    </tr>


    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='steer_image' />
          <img src='images/steer.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/steer.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://aclanthology.org/2023.findings-emnlp.506/" target=”_blank”>
         <span class="papertitle">STEER: Unified Style Transfer with Expert Reinforcement</span>
        </a>
        <br>
        <a href="https://skylerhallinan.com/" target=”_blank”>Skyler Hallinan</a>,
        <a href="https://fabrahman.github.io/" target=”_blank”>Faeze Brahman</a>,
        <a href="https://www.linkedin.com/in/ximing-lu-4aa9a51a0/" target=”_blank”>Ximing Lu</a>,
        <strong>Jaehun Jung</strong>,
        <a href="https://wellecks.com/" target=”_blank”>Sean Welleck</a>,
        <a href="https://homes.cs.washington.edu/~yejin/" target=”_blank”>Yejin Choi</a>
        <br>
        <em>Findings of EMNLP</em>, 2023
        <br>
        <a href="https://aclanthology.org/2023.findings-emnlp.506.pdf" target=”_blank”>paper</a>
        /
        <a href="https://github.com/shallinan1/STEERStyleTransfer" target=”_blank”>github</a>
        /
        <a href="data/steer.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          We propose a text style transfer framework from arbitrary source style to many target styles via large-scale data generation with expert-guided decoding and offline RL.
        </p>
      </td>
    </tr>


    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='maieutic_prompting_image' />
          <img src='images/maieutic_prompting.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/maieutic_prompting.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://aclanthology.org/2022.emnlp-main.82/" target=”_blank”>
          <span class="papertitle">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</span>
        </a>
        <br>
        <strong>Jaehun Jung</strong>,
        <a href="https://sites.google.com/view/lianhuiqin/home" target=”_blank”>Lianhui Qin</a>,
        <a href="https://wellecks.com/" target=”_blank”>Sean Welleck</a>,
        <a href="https://fabrahman.github.io/" target=”_blank”>Faeze Brahman</a>,
        <a href="https://www.chandrab.page/" target=”_blank”>Chandra Bhagavatula</a>,
        <a href="https://rlebras.github.io/" target=”_blank”>Ronan Le Bras</a>,
        <a href="https://homes.cs.washington.edu/~yejin/" target=”_blank”>Yejin Choi</a>
        <br>
        <em>EMNLP</em>, 2022 <strong style="color:red">(Oral Presentation)</strong>
        <br>
        <a href="https://aclanthology.org/2022.emnlp-main.82.pdf" target=”_blank”>paper</a>
        /
        <a href="https://github.com/jaehunjung1/Maieutic-Prompting" target=”_blank”>github</a>
        /
        <a href="data/maieutic_prompting.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          We improve LM reasoning by generating abductive and recursive explanations from language models, then formulating inference as a satisfiability problem over these generations.
        </p>
      </td>
    </tr>

    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='t-gap_image' />
          <img src='images/t-gap.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/t-gap.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://dl.acm.org/doi/10.1145/3447548.3467292" target=”_blank”>
          <span class="papertitle">Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion</span>
        </a>
        <br>
        <strong>Jaehun Jung</strong>,
        <a href="https://jinhongjung.github.io/" target=”_blank”>Jinhong Jung</a>,
        <a href="https://datalab.snu.ac.kr/~ukang/" target=”_blank”>U Kang</a>
        <br>
        <em>KDD</em>, 2021
        <br>
        <a href="https://jinhongjung.github.io/assets/resources/papers/tgapKDD21.pdf" target=”_blank”>paper</a>
        /
        <a href="https://github.com/jaehunjung1/T-GAP" target=”_blank”>github</a>
        /
        <a href="data/t-gap.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          A novel GNN for temporal KG is proposed that encodes an interpretable graph substructure for knowledge graph completion.
        </p>
      </td>
    </tr>


    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='attnio_image' />
          <img src='images/attnio.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/attnio.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://aclanthology.org/2020.emnlp-main.280/" target=”_blank”>
          <span class="papertitle">AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue</span>
        </a>
        <br>
        <strong>Jaehun Jung</strong>,
        Bokyung Son,
        Sungwon Lyu
        <br>
        <em>EMNLP</em>, 2020
        <br>
        <a href="https://aclanthology.org/2020.emnlp-main.280.pdf" target=”_blank”>paper</a>
        /
        <a href="https://slideslive.com/38938773" target=”_blank”>video</a>
        /
        <a href="data/attnio.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          We present a novel decoder model based on attention flow that learns to explore KG and retrieve a relevant knowledge path to ground a dialogue agent.
        </p>
      </td>
    </tr>


    <tr>
      <td class="image_part" style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='datahalo_image' />
          <img src='images/datahalo.png' style="width:100%;position:absolute;top:0;bottom:0;margin:auto">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <div class="image_device" style="align-content: center;margin-bottom: 10pt">
          <img src='images/datahalo.png' style="margin-left:auto;margin-right:auto;display:block;width:50%;top:0;bottom:0;">
        </div>
        <a href="https://dl.acm.org/doi/full/10.1145/3544548.3580828" target=”_blank”>
          <span class="papertitle">DataHalo: A Customizable Notification Visualization System for Personalized and Longitudinal Interactions</span>
        </a>
        <br>
        <a href="https://kr.linkedin.com/in/guhyun-han-830215285" target=”_blank”>Guhyun Han</a>,
        <strong>Jaehun Jung</strong>,
        <a href="http://younghokim.net/" target=”_blank”>Youngho Kim</a>
        <a href="http://hcil.snu.ac.kr/people/jinwook-seo" target=”_blank”>Jinwook Seo</a>
        <br>
        <em>CHI</em>, 2023
        <br>
        <a href="https://dl.acm.org/doi/pdf/10.1145/3544548.3580828" target=”_blank”>paper</a>
        /
        <a href="data/datahalo.txt" target=”_blank”>bibtex</a>
        <p></p>
        <p>
          DataHalo implements a customizable notification visualization system for mobile devices, providing prolonged ambient visualizations based on time-varying importance model to enable longitudinal interaction with the notifications.
        </p>
      </td>
    </tr>


    </tbody></table>

          <br><br>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website design by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
